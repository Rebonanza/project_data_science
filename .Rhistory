library(rtweet)
library(SnowballC)
library(httr)
#library(twitteR)
#library(Rstem)
library(textdata)
library(purrr)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
#library(sentiment)
#library(foo)
library(tidyverse)
library(tidytext)
library(e1071)
library(gmodels)
library(plyr)
library(plotly)
library(DT)
library(sass)
## store api keys (these are fake example values; replace with your own keys)
api_key <- "FBox1hjtTDLO3VZvyKWIZx77j"
api_secret_key <- "b3oOc7sGKHrzCdlHjFP8L6U5zWOdhSiEv3g3TJAfwQ02EV6IW5"
bearer_token <- "AAAAAAAAAAAAAAAAAAAAAF4YjgEAAAAADwzHHipo40WWiP%2F7NYjSaRvL5Gk%3DmLyBXX31ggRKxdEYkNmIid8uEKkzKiQ951aO1lJgE1qvzBC3JS"
clientId <- "MVoyLWVTMEVrWTd6OFUzTGxqbVI6MTpjaQ"
clientSecret <- "URnCd6sUHe4nieKbfnQ1xRJUwmhNOnfCLvXN7RioBY9w07_nPa"
accessToken <- "1592089064374534145-6hpvtXEwCUBMK7wbZCIXaRXTlarEWc"
accessSecret <- "h35r7DP0dR4AcUwBQqUXE5diaPTDkY7TEm4s41cT9X3xp"
token = create_token(
app = "sentimentAnalysisBooster",
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = accessToken,
access_secret = accessSecret,
set_renv = TRUE
)
auth_save(token, "tweetAccess")
auth_as("tweetAccess")
auth_sitrep()
dataVaksinCovid <- search_tweets('#vaksincovid19', n=500, include_rts=FALSE)
dataVaksinCovid <- search_tweets('#vaksincovid19', n=500, include_rts=FALSE)
dataVaksinBooster <- search_tweets('#vaksinbooster', n=500, include_rts=FALSE)
dataVaksin <- search_tweets('#vaksin', n=500, include_rts=FALSE)
dataBooster <- search_tweets('#booster', n=500, include_rts=FALSE)
#dataTweets <- sapply(dataTweet, function(x) x$getText())
data <- rbind(dataVaksinCovid, dataVaksin, dataVaksinBooster, dataBooster)
data <- data %>% select(full_text)
data$stripped_text1 <- gsub("http\\S+","",data$full_text)
data <- data %>%
select(stripped_text1) %>%
unnest_tokens(word, stripped_text1)
View(data)
View(data)
data$stripped_text1 <- gsub("http\\S+","",data$full_text)
cleanedData <- data %>%
anti_join(stop_words)
View(cleanedData)
class_emotion = classify_emotion(cleanedData, algorithm="bayes", prior=1.0)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
#library(twitteR)
#library(Rstem)
library(textdata)
library(purrr)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
library(sentiment)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
#library(twitteR)
#library(Rstem)
library(textdata)
library(purrr)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
library(sentiment)
install.packages("sentiment")
install.packages("caret")
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
#library(twitteR)
#library(Rstem)
library(textdata)
library(purrr)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
#library(sentiment)
#library(foo)
library(tidyverse)
library(tidytext)
library(e1071)
library(caret)
library(syuzhet)
library(gmodels)
library(plyr)
library(plotly)
library(DT)
library(sass)
# Menjadikan semua text menjadi lowercase atau huruf kecil
data1 = data
data <- tolower(data)
# replace blank space ("RT")
data <- gsub("rt", "", data)
# replace twitter @ username handle
data <- gsub("@\\w+", "", data)
# Menghilangkan tanda baca
data <- gsub("[[:punct:]]", "", data)
# Menghilangkan link
data <- gsub("http\\w+", "", data)
# Menghilangkan Tabs
data <- gsub("[ |\t]{2,}", "", data)
# Menghapus lahan kosong di awal tweet
data <- gsub("^ ", "", data)
# Menghapus lahan kosong di akhir tweet
data <- gsub(" $", "", data)
dataClean <- tm_map(dataClean, removeun)
knitr::opts_chunk$set(echo = TRUE)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
#library(twitteR)
#library(Rstem)
library(textdata)
library(purrr)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
#library(sentiment)
#library(foo)
library(tidyverse)
library(tidytext)
library(e1071)
library(caret)
library(syuzhet)
library(gmodels)
library(plyr)
library(plotly)
library(DT)
library(sass)
data1 <-  Corpus(VectorSource(data))
removeLink <- function(d) gsub("http[^[:space;]]*","",d)
dataClean <- tm_map(data1, removeLink)
View(data1)
data1 <-  Corpus(VectorSource(data))
removeLink <- function(d) gsub("http[^[:space]]*","",d)
dataClean <- tm_map(data1, removeLink)
data <- rbind(dataVaksinCovid, dataVaksin, dataVaksinBooster, dataBooster)
data <- data %>% select(full_text)
data1 <-  Corpus(VectorSource(data$full_text))
removeLink <- function(d) gsub("http[^[:space]]*","",d)
dataClean <- tm_map(data1, removeLink)
data1 <-  Corpus(VectorSource(data$full_text))
removeLink <- function(d) gsub("http[^[:space:]]*","",d)
dataClean <- tm_map(data1, removeLink)
removenl <- function(d) gsub("\n"," ",d)
dataClean <- tm_map(dataClean, removenl)
removeComma <- function(d) gsub(",","",d)
dataClean <- tm_map(dataClean, removeComma)
removeTitik2 <- function(d) gsub(":","",d)
dataClean <- tm_map(dataClean, removTitik2)
data1 <-  Corpus(VectorSource(data$full_text))
removeLink <- function(d) gsub("http[^[:space:]]*","",d)
dataClean <- tm_map(data1, removeLink)
removenl <- function(d) gsub("\n"," ",d)
dataClean <- tm_map(dataClean, removenl)
removeComma <- function(d) gsub(",","",d)
dataClean <- tm_map(dataClean, removeComma)
removeTitik2 <- function(d) gsub(":","",d)
dataClean <- tm_map(dataClean, removeTitik2)
removeTitikKoma <- function(d) gsub(";","",d)
dataClean <- tm_map(dataClean, removeTitikKoma)
removeAmp <- function(d) gsub("&amp","",d)
dataClean <- tm_map(dataClean, removeAmp)
removeun <- function(d) gsub("@\\w+","",d)
dataClean <- tm_map(dataClean, removeun)
remove.all <- function(d) gsub("[^[:alpha:][:space:]]","",d)
dataClean <- tm_map(dataClean, remove.all)
dataClean <- tm_map(dataClean, removePunctuation)
dataClean <- tm_map(dataClean, tolower)
df <- data.frame(text=unlist(sapply(dataClean,'[')),stringAsFactors=F)
View(df)
View(df)
data1 <-  Corpus(VectorSource(data$full_text))
removeLink <- function(d) gsub("http[^[:space:]]*","",d)
dataClean <- tm_map(data1, removeLink)
removenl <- function(d) gsub("\n"," ",d)
dataClean <- tm_map(dataClean, removenl)
removeComma <- function(d) gsub(",","",d)
dataClean <- tm_map(dataClean, removeComma)
removeTitik2 <- function(d) gsub(":","",d)
dataClean <- tm_map(dataClean, removeTitik2)
removeTitikKoma <- function(d) gsub(";","",d)
dataClean <- tm_map(dataClean, removeTitikKoma)
removeAmp <- function(d) gsub("&amp","",d)
dataClean <- tm_map(dataClean, removeAmp)
removeun <- function(d) gsub("@\\w+","",d)
dataClean <- tm_map(dataClean, removeun)
remove.all <- function(d) gsub("[^[:alpha:][:space:]]","",d)
dataClean <- tm_map(dataClean, remove.all)
dataClean <- tm_map(dataClean, removePunctuation)
dataClean <- tm_map(dataClean, tolower)
df <- data.frame(text=unlist(sapply(dataClean,'[')),stringAsFactors=F)
write.csv(df,file="dataTwitterClean.csv")
dataChar <- as.character(df$text)
sentiment <- get_nrc_sentiment(dataChar)
mergeData <- cbind(df$text,sentiment)
par(mar=rep(3,4))
View(mergeData)
install.packages("RTextTools")
knitr::opts_chunk$set(echo = TRUE)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
library(Rtexttools)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
library(RtextTools)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
library(RtextTools)
library(openssl)
library(httpuv)
library(rtweet)
library(SnowballC)
library(httr)
library(RTextTools)
#library(twitteR)
#library(Rstem)
library(textdata)
library(purrr)
library(tm)
library(NLP)
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
#library(sentiment)
#library(foo)
library(tidyverse)
library(tidytext)
library(e1071)
library(caret)
library(syuzhet)
library(gmodels)
library(plyr)
library(plotly)
library(DT)
library(sass)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
View(newData)
View(newData)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
View(corpus)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
View(corpus.clean)
View(corpus)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
View(dtm.test)
View(newData.train)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
View(dtm.test.nb)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
convetCount <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels = c(0,1),labels = c=("no","yes"))
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
convetCount <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels = c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convertCount)
newData <- read.csv(file="dataTwitterClean.csv")
glimpse(newData)
set.seed(20)
newData<-newData[sample(nrow(newData)),]
newData<-newData[sample(nrow(newData)),]
glimpse(newData)
corpus<-Corpus(VectorSource(newData$text))
corpus
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map((removeNumbers))%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
newData.train <- newData[1:50,]
newData.test <- newData[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:64,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
convertCount <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels = c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convertCount)
View(trainNB)
